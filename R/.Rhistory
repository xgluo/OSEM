S <- solve(precisions[,,a])
mus <- rbind(mus, rmvnorm(1, c(-1,1), S))
}
# Draw obseration
observations[i,] <- rmvnorm(1, mus[a,], S)
}
# Summary
length(clusters)
table(assignments)
#### Plot results
df <- data.frame(cbind(observations,assignments))
colnames(df) <- c("X","Y","Cluster")
df$Cluster <- as.factor(df$Cluster)
ggplot(df, aes(x = X, y = Y, color = Cluster)) + geom_point() + scale_color_brewer(palette="Paired")
########## Simulation -- Chinese Restaurant Process
library(mvtnorm)
library(abind)
library(ggplot2)
#### Initialization
set.seed(666)
N <- 10 # total number of customers in simulation
assignments <- c(1, rep(0, N-1)) # cluster/table assignments (colors)
clusters <- c(1) # number of customers in each cluster
# Normal-Wishart prior on cluster parameters:
precisions <- rWishart(1, 4, diag(2))
S <- solve(precisions[,,1]) # invert to get the covariance
mus <- matrix(rmvnorm(1, c(-1,1), S),nrow = 1, ncol = 2)
# customer observations (coordinates)
observations <- matrix(0, nrow = N, ncol = 2)
observations[1,] <- rmvnorm(1, mus[1,], S)
# Concentration parameter
alpha <- 1
#### Simulation
for (i in c(2:N)) {
# Assign cluster
probs <- c(clusters, alpha) / (sum(clusters) + alpha)
a <- sample.int(length(clusters) + 1, size = 1, prob = probs)
assignments[i] <- a
# Determine cluster parameters
if (a <= length(clusters)) { # take existing ones
clusters[a] <- clusters[a] + 1
S <- solve(precisions[,,a])
} else { # create new ones
clusters <- c(clusters, 1)
precisions <- abind(precisions, rWishart(1, 3, diag(2)), along = 3)
S <- solve(precisions[,,a])
mus <- rbind(mus, rmvnorm(1, c(-1,1), S))
}
# Draw obseration
observations[i,] <- rmvnorm(1, mus[a,], S)
}
# Summary
length(clusters)
table(assignments)
#### Plot results
df <- data.frame(cbind(observations,assignments))
colnames(df) <- c("X","Y","Cluster")
df$Cluster <- as.factor(df$Cluster)
ggplot(df, aes(x = X, y = Y, color = Cluster)) + geom_point() + scale_color_brewer(palette="Paired")
########## Simulation -- Chinese Restaurant Process
library(mvtnorm)
library(abind)
library(ggplot2)
#### Initialization
set.seed(666)
N <- 1000 # total number of customers in simulation
assignments <- c(1, rep(0, N-1)) # cluster/table assignments (colors)
clusters <- c(1) # number of customers in each cluster
# Normal-Wishart prior on cluster parameters:
precisions <- rWishart(1, 4, diag(2))
S <- solve(precisions[,,1]) # invert to get the covariance
mus <- matrix(rmvnorm(1, c(-1,1), S),nrow = 1, ncol = 2)
# customer observations (coordinates)
observations <- matrix(0, nrow = N, ncol = 2)
observations[1,] <- rmvnorm(1, mus[1,], S)
# Concentration parameter
alpha <- 1
#### Simulation
for (i in c(2:N)) {
# Assign cluster
probs <- c(clusters, alpha) / (sum(clusters) + alpha)
a <- sample.int(length(clusters) + 1, size = 1, prob = probs)
assignments[i] <- a
# Determine cluster parameters
if (a <= length(clusters)) { # take existing ones
clusters[a] <- clusters[a] + 1
S <- solve(precisions[,,a])
} else { # create new ones
clusters <- c(clusters, 1)
precisions <- abind(precisions, rWishart(1, 3, diag(2)), along = 3)
S <- solve(precisions[,,a])
mus <- rbind(mus, rmvnorm(1, c(-1,1), S))
}
# Draw obseration
observations[i,] <- rmvnorm(1, mus[a,], S)
}
# Summary
length(clusters)
table(assignments)
#### Plot results
df <- data.frame(cbind(observations,assignments))
colnames(df) <- c("X","Y","Cluster")
df$Cluster <- as.factor(df$Cluster)
ggplot(df, aes(x = X, y = Y, color = Cluster)) + geom_point() + scale_color_brewer(palette="Paired")
########## Simulation -- Chinese Restaurant Process
library(mvtnorm)
library(abind)
library(ggplot2)
#### Initialization
set.seed(666)
N <- 1000 # total number of customers in simulation
assignments <- c(1, rep(0, N-1)) # cluster/table assignments (colors)
clusters <- c(1) # number of customers in each cluster
# Normal-Wishart prior on cluster parameters:
precisions <- rWishart(1, 4, diag(2))
S <- solve(precisions[,,1]) # invert to get the covariance
mus <- matrix(rmvnorm(1, c(-1,1), S),nrow = 1, ncol = 2)
# customer observations (coordinates)
observations <- matrix(0, nrow = N, ncol = 2)
observations[1,] <- rmvnorm(1, mus[1,], S)
# Concentration parameter
alpha <- 0.5
#### Simulation
for (i in c(2:N)) {
# Assign cluster
probs <- c(clusters, alpha) / (sum(clusters) + alpha)
a <- sample.int(length(clusters) + 1, size = 1, prob = probs)
assignments[i] <- a
# Determine cluster parameters
if (a <= length(clusters)) { # take existing ones
clusters[a] <- clusters[a] + 1
S <- solve(precisions[,,a])
} else { # create new ones
clusters <- c(clusters, 1)
precisions <- abind(precisions, rWishart(1, 3, diag(2)), along = 3)
S <- solve(precisions[,,a])
mus <- rbind(mus, rmvnorm(1, c(-1,1), S))
}
# Draw obseration
observations[i,] <- rmvnorm(1, mus[a,], S)
}
# Summary
length(clusters)
table(assignments)
#### Plot results
df <- data.frame(cbind(observations,assignments))
colnames(df) <- c("X","Y","Cluster")
df$Cluster <- as.factor(df$Cluster)
ggplot(df, aes(x = X, y = Y, color = Cluster)) + geom_point() + scale_color_brewer(palette="Paired")
# Load data
load("yeastStorey.rda")
# Design matrix
X <- as.matrix(data[,-1])
# Response variable
y <- as.matrix(data[,1])
# Training and test sets
set.seed(522)
# train.idx <- sample(nrow(data),nrow(data)*0.7)
train.idx <- createDataPartition(seq(1,nrow(data)),1,0.7,list = FALSE)
X.train <- X[train.idx,]
X.test <- X[-train.idx,]
y.train <- y[train.idx]
y.test <- y[-train.idx]
set.seed(123)
X_data <- data[,2:ncol(data)]
y_data <- data$Marker
index <- createDataPartition(y = data$Marker, p = .70, list = FALSE)
X_train <- data.matrix(X_data[ index, ])
X_test <- data.matrix(X_data[-index, ])
y_train <- y_data[index]
y_test<- y_data[-index]
alpha_list <- seq(0,1,by=0.1)
elasticnet <- lapply(alpha_list, function(a){
cv.glmnet(X_train, y_train, alpha=a, nfolds=10, family="binomial", type.measure="mse")
})
min_alphas = c()
for (i in 1:11) {
print(min(elasticnet[[i]]$cvm))
min_alphas = c(min_alphas, min(elasticnet[[i]]$cvm)) }
index_optimal_alpha = which.min(min_alphas)
optimal_alpha <- alpha_list[index_optimal_alpha]
index_optimal_lambda <- which.min(elasticnet[[index_optimal_alpha]]$cvm)
optimal_lambda <- elasticnet[[index_optimal_alpha]]$lambda[index_optimal_lambda]
fitcv <- cv.glmnet(X_train, y_train, alpha=optimal_alpha, type.measure = "mse", nfolds=10, type = "binomial")
fitcv <- cv.glmnet(X_train, y_train, alpha=optimal_alpha, type.measure = "mse", nfolds=10, family = "binomial")
y_predict <- predict(fitcv, newx=X_test, s="lambda.min", type="response")
y_predict
########### Psychological Application ###########
# This file computes the log-loss on test data
# Required packages
library(BiDAG)
library(pcalg)
setwd("~/Documents/Projects/OSEM/OSEM/R/")
# Major file containing the OSEM algorithm
source("ordinalScore.R")
# Modify some of the existing functions in the BiDAG package to accommodate our user-defined functions
insertSource("spacefns.R",package = "BiDAG")
insertSource("usrscorefns.R",package = "BiDAG")
insertSource("initpar.R",package = "BiDAG")
insertSource("scoreagainstdag.R",package = "BiDAG")
# Load data
setwd("~/Documents/Projects/OSEM/OSEM/psych_application/")
load("OCDRogers.RData")
n <- ncol(datRogers)
N <- nrow(datRogers)
# Create training and test set for validation set approach (80% train/20% test)
test <- sample(1:N, N/5, replace=FALSE)
train <- (-test)
train.data <- datRogers[train,]
test.data <- datRogers[test,]
datRogers_levels <- apply(datRogers, 2, function(x) length(unique(x)))
datRogers_levels - apply(train.data, 2, function(x) length(unique(x)))
OSEMfit <- ordinalStructEM(n, train.data,
usrpar = list(penType = "other",
L = 5,
lambda = 4,
preLevels = datRogers_levels))
OSEM.test.param <- OSEMfit$param
OSEM.test.param$data <- test.data
observedLL(OSEM.test.param)
# Major file containing the OSEM algorithm
source("ordinalScore.R")
# Required packages
library(BiDAG)
library(pcalg)
setwd("~/Documents/Projects/OSEM/OSEM/R/")
# Major file containing the OSEM algorithm
source("ordinalScore.R")
# Modify some of the existing functions in the BiDAG package to accommodate our user-defined functions
insertSource("spacefns.R",package = "BiDAG")
insertSource("usrscorefns.R",package = "BiDAG")
insertSource("initpar.R",package = "BiDAG")
insertSource("scoreagainstdag.R",package = "BiDAG")
BGE <- scoreparameters("bge", train.data, bgepar = list(am = 1))
BGEfit <- iterativeMCMC(BGE)
mean(apply(BGEfit$DAG,2,sum))
BGE.test.param <- BGE
#BGE.test.param$Sigma_hat <- cov2cor(BGE$SigmaN)
#BGE.test.param$Sigma_hat <- cov2cor(covforDAG(BGE$TN,BGEfit$DAG))
BGE.test.param$Sigma_hat <- BGe_MAP_Sigma(BGE$TN,BGEfit$DAG)
BGE.test.param$cuts <- OSEM.test.param$cuts
BGE.test.param$data <- test.data
observedLL(BGE.test.param)
sum(scoreagainstDAG(BGE,BGEfit$DAG,test.data))
#sum(scoreagainstDAG(BGE,BGEfit$DAG,test.data))
mean(apply(BGEfit$DAG,2,sum))
mean(apply(OSEMfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 20,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 40,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 50,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
?roc
?mcapply
library(parallel)
?mcapply
?mclapply
# OSEM
OSEMfit <- ordinalStructEM(n, train.data,
usrpar = list(penType = "other",
L = 5,
lambda = 1,
preLevels = datRogers_levels))
mean(apply(OSEMfit$DAG,2,sum))
OSEM.test.param <- OSEMfit$param
OSEM.test.param$data <- test.data
observedLL(OSEM.test.param)
tempgraph <- graphAM(OSEMfit$DAG, edgemode = "directed")
plot(tempgraph, main= "OSEM")
BGE <- scoreparameters("bge", train.data, bgepar = list(am = 1))
BGEfit <- iterativeMCMC(BGE)
mean(apply(BGEfit$DAG,2,sum))
BGE.test.param <- BGE
#BGE.test.param$Sigma_hat <- cov2cor(BGE$SigmaN)
#BGE.test.param$Sigma_hat <- cov2cor(covforDAG(BGE$TN,BGEfit$DAG))
BGE.test.param$Sigma_hat <- BGe_MAP_Sigma(BGE$TN,BGEfit$DAG)
BGE.test.param$cuts <- OSEM.test.param$cuts
BGE.test.param$data <- test.data
observedLL(BGE.test.param)
#sum(scoreagainstDAG(BGE,BGEfit$DAG,test.data))
mean(apply(BGEfit$DAG,2,sum))
tempgraph <- graphAM(BGEfit$DAG, edgemode = "directed")
plot(tempgraph, main= "BGe")
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 20,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 35,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 100,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 200,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
tempgraph <- graphAM(BDEfit$DAG, edgemode = "directed")
plot(tempgraph, main= "BDe")
50/5
50/25
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 120,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 50,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
BDE <- scoreparameters("bdecat",train.data,bdecatpar = list(chi = 100,bdecatCvec=datRogers_levels))
BDEfit <- iterativeMCMC(BDE)
sum(scoreagainstDAG(BDE,BDEfit$DAG,test.data,bdecatCvec=datRogers_levels))
mean(apply(BDEfit$DAG,2,sum))
50/5
60/5
BGE <- scoreparameters("bge", train.data, bgepar = list(am = 0.05))
BGEfit <- iterativeMCMC(BGE)
mean(apply(BGEfit$DAG,2,sum))
BGE.test.param <- BGE
#BGE.test.param$Sigma_hat <- cov2cor(BGE$SigmaN)
#BGE.test.param$Sigma_hat <- cov2cor(covforDAG(BGE$TN,BGEfit$DAG))
BGE.test.param$Sigma_hat <- BGe_MAP_Sigma(BGE$TN,BGEfit$DAG)
BGE.test.param$cuts <- OSEM.test.param$cuts
BGE.test.param$data <- test.data
observedLL(BGE.test.param)
#sum(scoreagainstDAG(BGE,BGEfit$DAG,test.data))
mean(apply(BGEfit$DAG,2,sum))
res <- matrix(nrow = 2,ncol = 5)
am_list <- c(0.05,0.1,0.25,0.5,1)
for (i in c(1:length(am_list))) {
BGE <- scoreparameters("bge", train.data, bgepar = list(am = am_list[i]))
BGEfit <- iterativeMCMC(BGE)
mean(apply(BGEfit$DAG,2,sum))
BGE.test.param <- BGE
#BGE.test.param$Sigma_hat <- cov2cor(BGE$SigmaN)
#BGE.test.param$Sigma_hat <- cov2cor(covforDAG(BGE$TN,BGEfit$DAG))
BGE.test.param$Sigma_hat <- BGe_MAP_Sigma(BGE$TN,BGEfit$DAG)
BGE.test.param$cuts <- OSEM.test.param$cuts
BGE.test.param$data <- test.data
res[1,i] <- observedLL(BGE.test.param)
#sum(scoreagainstDAG(BGE,BGEfit$DAG,test.data))
res[2,i] <- mean(apply(BGEfit$DAG,2,sum))
}
res
res <- matrix(nrow = 2,ncol = 5)
am_list <- c(2,3,4,5,6)
for (i in c(1:length(am_list))) {
BGE <- scoreparameters("bge", train.data, bgepar = list(am = am_list[i]))
BGEfit <- iterativeMCMC(BGE)
mean(apply(BGEfit$DAG,2,sum))
BGE.test.param <- BGE
#BGE.test.param$Sigma_hat <- cov2cor(BGE$SigmaN)
#BGE.test.param$Sigma_hat <- cov2cor(covforDAG(BGE$TN,BGEfit$DAG))
BGE.test.param$Sigma_hat <- BGe_MAP_Sigma(BGE$TN,BGEfit$DAG)
BGE.test.param$cuts <- OSEM.test.param$cuts
BGE.test.param$data <- test.data
res[1,i] <- observedLL(BGE.test.param)
#sum(scoreagainstDAG(BGE,BGEfit$DAG,test.data))
res[2,i] <- mean(apply(BGEfit$DAG,2,sum))
}
res
opt.pcart.cat.internal <- function(data, predictors, response, type, param, use_structure_score) {
if(class(data) != "data.frame") {
stop("data should be a data.frame")
}
if(length(response) != 1) {
stop("response should have length 1")
}
vars <- c(predictors, response)
if(any(duplicated(vars))) {
stop("no column label should appear twice in c(predictors, response)")
}
mydata <- data.frame(lapply(data[vars], to.factor))
types <- c(rep("CAT", length(predictors)), type)
varstr <- paste(types, unlist(lapply(lapply(mydata, levels), length)), collapse=" ")
varstr <- paste(varstr, param)
datastr <- paste(t(data.matrix(mydata)) - 1, sep=' ', collapse=' ')
flagstr <- ""
if(!use_structure_score) {
flagstr <- "%NO_USE_STRUCTURE_SCORE"
}
input <- paste(flagstr, length(predictors), nrow(data), varstr, datastr)
# callresult <- call.pcartcli(input)
# score <- callresult$score
# tree <- callresult$tree
#
# pos <- 0
# prettify <- function(pre1, pre) {
#   pos <<- pos + 1
#   line <- strsplit(tree[[pos]], " +")[[1]]
#   if(line[1] == "SPLIT") {
#     varidx <- as.numeric(line[3]) + 1
#     cats <- line[4:(length(line)-1)]
#     cats[cats == -1] = "|"
#     tree[[pos]] <<- paste0(pre1, "-+ ", predictors[varidx], ": ", paste(cats, collapse=" "))
#     prettify(paste0(pre, " |"), paste0(pre, " |"))
#     prettify(paste0(pre, " `"), paste0(pre, "  "))
#   } else {
#     tree[[pos]] <<- paste0(pre1, "-- ", response, ": ", paste(line[3:length(line)], collapse=" "))
#   }
# }
# prettify("", "")
#
# tree <- paste0(paste(tree, collapse="\n"), "\n")
#
# return(list(score=score, tree=tree))
return(input)
}
opt.pcart.ord <- function(data, predictors, response, alpha=0.5, use_structure_score=TRUE) {
return(opt.pcart.cat.internal(data, predictors, response, "ORD", alpha, use_structure_score))
}
opt.pcart.ord(train.data,c(1),c(2,3))
opt.pcart.ord(train.data,c(1,2),3)
to.factor <- function(x) {
# Ensures that missing levels are not removed
if(class(x) == "factor") {
return(x)
} else {
return(factor(x))
}
}
opt.pcart.ord(train.data,c(1,2),3)
data <- data.frame(
a=c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1),
b=c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1),
c=c(0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2)
)
result <- opt.pcart.ord(data, c("a", "b"), "c")
result
score <- -5.527752992437118
score
tree <- "LEAF ORD 4 8 4"
os <- 0
prettify <- function(pre1, pre) {
pos <<- pos + 1
line <- strsplit(tree[[pos]], " +")[[1]]
if(line[1] == "SPLIT") {
varidx <- as.numeric(line[3]) + 1
cats <- line[4:(length(line)-1)]
cats[cats == -1] = "|"
tree[[pos]] <<- paste0(pre1, "-+ ", predictors[varidx], ": ", paste(cats, collapse=" "))
prettify(paste0(pre, " |"), paste0(pre, " |"))
prettify(paste0(pre, " `"), paste0(pre, "  "))
} else {
tree[[pos]] <<- paste0(pre1, "-- ", response, ": ", paste(line[3:length(line)], collapse=" "))
}
}
prettify("", "")
tree <- paste0(paste(tree, collapse="\n"), "\n")
pos <- 0
prettify <- function(pre1, pre) {
pos <<- pos + 1
line <- strsplit(tree[[pos]], " +")[[1]]
if(line[1] == "SPLIT") {
varidx <- as.numeric(line[3]) + 1
cats <- line[4:(length(line)-1)]
cats[cats == -1] = "|"
tree[[pos]] <<- paste0(pre1, "-+ ", predictors[varidx], ": ", paste(cats, collapse=" "))
prettify(paste0(pre, " |"), paste0(pre, " |"))
prettify(paste0(pre, " `"), paste0(pre, "  "))
} else {
tree[[pos]] <<- paste0(pre1, "-- ", response, ": ", paste(line[3:length(line)], collapse=" "))
}
}
prettify("", "")
tree <- paste0(paste(tree, collapse="\n"), "\n")
tree
tree <- "LEAF ORD 4 8 4"
pos = 0
trsplit(tree[[pos]], " +")[[1]]
strsplit(tree[[pos]], " +")[[1]]
pos <<- pos + 1
pos = 1
strsplit(tree[[pos]], " +")[[1]]
line <- strsplit(tree[[pos]], " +")[[1]]
tree[[pos]] <<- paste0(pre1, "-- ", response, ": ", paste(line[3:length(line)], collapse=" "))
tree[[pos]] <<- paste0("", "-- ", response, ": ", paste(line[3:length(line)], collapse=" "))
tree
tree[[1]]
paste0("", "-- ", response, ": ", paste(line[3:length(line)], collapse=" "))
opt.pcart.ord(train.data,c(1,2),3)
OSEMfit$DAG
opt.pcart.ord(train.data,c(5,9),3)
opt.pcart.ord(train.data,c(),3)
opt.pcart.ord(train.data,c(5,7,9),3)
opt.pcart.ord(train.data,c(5,9),3)
which(rownames(BDEfit$DAG) == "weight")
opt.pcart.ord(train.data,14,13)
opt.pcart.ord(train.data,13,14)
library(treeMHN)
treeMHN:::rtexp(1,0,1)
library(rpcart)
library(rpcart)
remove.packages(rpcart)
remove.packages("rpcart")
library(rpcart)
remove.packages("rpcart")
library(rpcart)
library(rpcart)
library(rpcart)
remove.packages("rpcart")
library(rpcart)
